gantry run \
  --name "ow-train-0.00175" \
  --description "Hyperparameter tuning (0.00175)" \
  --allow-dirty \
  --no-nfs \
  --preemptible \
  --workspace ai2/open-whisper \
  --cluster ai2/neptune-cirrascale \
  --cpus 248 \
  --gpus 8 \
  --beaker-image huongn/ow_train_gantry \
  --pip requirements-main.txt \
  --budget ai2/prior \
  --weka oe-data-default:/weka \
  --dataset huongn/mini-job-ow-evalset:/ow_eval \
  --env-secret WANDB_API_KEY=WANDB_API_KEY \
  --env WANDB_DIR=/results/huongn/ow_logs \
  --env TORCH_NCCL_BLOCKING_WAIT=1 \
  --env NCCL_DEBUG=INFO \
  --priority normal \
  -- /bin/bash -c "torchrun --nnodes 1 --nproc_per_node 8 scripts/training/train_wds.py \
      --model_variant=tiny \
      --exp_name=ow_tiny_wds_0.00175 \
      --job_type=hyperparam \
      --train_shards=/weka/huongn/ow_440K_wds/\{000000..073467\}.tar \
      --train_steps=9375 \
      --val_shards=/weka/huongn/ow_440K_wds/073468.tar \
      --run_id=None \
      --ckpt_file_name=None \
      --ckpt_dir=/weka/huongn/ow_ckpts \
      --log_dir=/results/huongn/ow_logs \
      --eval_dir=/ow_eval \
      --rank=None \
      --world_size=None \
      --lr=1.75e-3 \
      --betas='(0.9, 0.98)' \
      --eps=1e-6 \
      --weight_decay=0.1 \
      --max_grad_norm=1.0 \
      --eff_batch_size=256 \
      --train_batch_size=16 \
      --val_batch_size=16 \
      --eval_batch_size=16 \
      --num_workers=8 \
      --pin_memory=True \
      --persistent_workers=True \
      --run_val=True \
      --run_eval=True \
      --train_log_freq=50 \
      --val_freq=10 \
      --eval_freq=5"
  
