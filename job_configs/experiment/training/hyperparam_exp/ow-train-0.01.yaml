version: v2
budget: ai2/prior
description: Hyperparameter tuning (0.01)
tasks:
  - name: ow_train_0.01
    image:
      beaker: huongn/ow_train_hyperparam
    command: ['/bin/bash', '-c']
    arguments: ['torchrun --nnodes 1 --nproc_per_node 8 scripts/training/train_wds.py
      --model_variant=tiny
      --exp_name=ow_tiny_wds_0.01
      --job_type=hyperparam
      --train_shards=/weka/huongn/ow_440K_wds/\{000000..073467\}.tar
      --train_steps=9375
      --val_shards=/weka/huongn/ow_440K_wds/073468.tar
      --run_id=None
      --ckpt_file_name=None
      --log_dir=/data/huongn/ow_logs
      --eval_dir=/ow_eval
      --rank=None
      --world_size=None
      --lr=1e-2
      --betas="(0.9, 0.98)"
      --eps=1e-6
      --weight_decay=0.1
      --max_grad_norm=1.0
      --eff_batch_size=2048
      --train_batch_size=8
      --val_batch_size=8
      --eval_batch_size=8
      --num_workers=8
      --pin_memory=True
      --persistent_workers=True
      --run_val=False
      --run_eval=False']
    datasets:
    - mountPath: /weka
      source:
        weka: oe-data-default
    - mountPath: /ow_eval
      source:
        beaker: huongn/mini-job-ow-evalset
    result:
      # Beaker will capture anything that's written to this location and store it in the results
      # dataset. This location is required to be a directory, not a file.
      path: /data/huongn/ow_logs
    envVars:
    - name: PYTHONPATH
      value: /stage
    - name: WANDB_API_KEY
      secret: WANDB_API_KEY
    - name: WANDB_DIR
      value: /data/huongn/ow_logs
    - name: TORCH_NCCL_BLOCKING_WAIT
      value: 1
    - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
      value:
    - name: NCCL_DEBUG
      value: INFO
    resources:
      cpuCount: 64
      gpuCount: 8
      memory: 20GiB
    context:
      priority: normal
      preemptible: True
    constraints:
      cluster: [ ai2/neptune-cirrascale ]