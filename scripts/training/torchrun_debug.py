from torch.distributed.run import main as torch_run

if __name__ == "__main__":
    command = [
        "--nnodes=1",
        "--nproc_per_node=2",
        "--master-port=29504",
        "scripts/training/train_fsdp.py",
        "--model_variant=small",
        "--exp_name=train_debug",
        "--job_type=debug",
        "--samples_dicts_dir=/weka/huongn/samples_dicts/filtered/mixed_no_repeat_min_comma_period_1_2",
        "--train_steps=206699",
        "--epoch_steps=206699",
        "--ckpt_file_name=None",
        "--ckpt_dir=/weka/huongn/ow_ckpts",
        "--log_dir=/results/huongn/ow_logs",
        "--eval_dir=/ow_eval",
        "--run_id_dir=/weka/huongn/ow_run_ids",
        "--rank=None",
        "--world_size=None",
        "--lr=1.5e-3",
        "--betas=(0.9, 0.98)",
        "--eps=1e-6",
        "--weight_decay=0.1",
        "--max_grad_norm=1.0",
        "--eff_batch_size=256",
        "--train_batch_size=8",
        "--val_batch_size=8",
        "--eval_batch_size=8",
        "--train_val_split=1.0",
        "--num_workers=4",
        "--pin_memory=True",
        "--persistent_workers=True",
        "--run_val=False",
        "--run_eval=False",
        "--train_log_freq=1454",
        "--val_freq=7273",
        "--eval_freq=7273",
        "--ckpt_freq=2",
]
    torch_run(command)