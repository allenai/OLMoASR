{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.tokenizer import get_tokenizer\n",
    "from whisper import audio\n",
    "import olmoasr as oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no timestamps\n",
    "# trim audio\n",
    "from typing import Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "def preprocess_audio(audio_file: str, norm_end: Optional[str]) -> Tuple[str, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the audio data for the model.\n",
    "\n",
    "    Loads the audio file, pads or trims the audio data, and computes the log mel spectrogram.\n",
    "\n",
    "    Args:\n",
    "        audio_file: The path to the audio file\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the name of audio file and the log mel spectrogram\n",
    "    \"\"\"\n",
    "    audio_arr = np.load(audio_file).astype(np.float32) / 32768.0\n",
    "    if norm_end:\n",
    "        # number of samples to trim until\n",
    "        length = oa.utils.convert_to_milliseconds(norm_end) * 16\n",
    "        # trim until end of text segment\n",
    "        audio_arr = audio.pad_or_trim(audio_arr, length=length)\n",
    "        # pad w/ silence\n",
    "        audio_arr = audio.pad_or_trim(audio_arr)\n",
    "    else:\n",
    "        # in case audio_arr isn't exactly 480K samples\n",
    "        audio_arr = audio.pad_or_trim(audio_arr)\n",
    "    mel_spec = audio.log_mel_spectrogram(audio_arr)\n",
    "\n",
    "    return mel_spec, audio_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"1\n",
    "00:00:00,000 --> 00:00:03,090\n",
    "Men talk more in\n",
    "male/female interaction\n",
    "\n",
    "2\n",
    "00:00:03,090 --> 00:00:06,110\n",
    "and they interrupt all the time.\n",
    "It's partly linked to poaer.\n",
    "\n",
    "3\n",
    "00:00:06,110 --> 00:00:09,510\n",
    "In general people who think they're\n",
    "more poaerful interrupt more.\n",
    "\n",
    "4\n",
    "00:00:10,950 --> 00:00:14,110\n",
    "So that would be something that\n",
    "a sociolinguist would discover,\n",
    "\n",
    "5\n",
    "00:00:14,110 --> 00:00:16,230\n",
    "different social groups\n",
    "speak somewhat differently\n",
    "\n",
    "6\n",
    "00:00:16,870 --> 00:00:19,810\n",
    "and there are people who look\n",
    "at you knoa computational linguistics\n",
    "\n",
    "7\n",
    "00:00:19,810 --> 00:00:22,980\n",
    "and the way you can use language\n",
    "in artificial intelligence,\n",
    "\n",
    "8\n",
    "00:00:23,680 --> 00:00:26,830\n",
    "teach machines to better\n",
    "be able to translate.\n",
    "\n",
    "9\n",
    "00:00:28,420 --> 00:00:29,200\n",
    "Many other areas.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, _, transcript_end = reader.read()\n",
    "    \n",
    "    transcript_text = reader.extract_text(transcript=transcript)\n",
    "    text_tokens = tokenizer.encode(transcript_text)\n",
    "    \n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(multilingual=False)\n",
    "no_timestamps_text_tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10418,\n",
       " 1561,\n",
       " 517,\n",
       " 287,\n",
       " 198,\n",
       " 22606,\n",
       " 14,\n",
       " 24724,\n",
       " 10375,\n",
       " 290,\n",
       " 484,\n",
       " 11313,\n",
       " 477,\n",
       " 262,\n",
       " 640,\n",
       " 13,\n",
       " 198,\n",
       " 1026,\n",
       " 338,\n",
       " 11476,\n",
       " 6692,\n",
       " 284,\n",
       " 1176,\n",
       " 13,\n",
       " 554,\n",
       " 2276,\n",
       " 661,\n",
       " 508,\n",
       " 892,\n",
       " 484,\n",
       " 821,\n",
       " 198,\n",
       " 3549,\n",
       " 3665,\n",
       " 11313,\n",
       " 517,\n",
       " 13,\n",
       " 1406,\n",
       " 326,\n",
       " 561,\n",
       " 307,\n",
       " 1223,\n",
       " 326,\n",
       " 198,\n",
       " 64,\n",
       " 1307,\n",
       " 1669,\n",
       " 6680,\n",
       " 396,\n",
       " 561,\n",
       " 7073,\n",
       " 11,\n",
       " 1180,\n",
       " 1919,\n",
       " 2628,\n",
       " 198,\n",
       " 47350,\n",
       " 6454,\n",
       " 10338,\n",
       " 290,\n",
       " 612,\n",
       " 389,\n",
       " 661,\n",
       " 508,\n",
       " 804,\n",
       " 198,\n",
       " 265,\n",
       " 345,\n",
       " 760,\n",
       " 31350,\n",
       " 20280,\n",
       " 3969,\n",
       " 290,\n",
       " 262,\n",
       " 835,\n",
       " 345,\n",
       " 460,\n",
       " 779,\n",
       " 3303,\n",
       " 198,\n",
       " 259,\n",
       " 11666,\n",
       " 4430,\n",
       " 11,\n",
       " 4545,\n",
       " 8217,\n",
       " 284,\n",
       " 1365,\n",
       " 198,\n",
       " 1350,\n",
       " 1498,\n",
       " 284,\n",
       " 15772,\n",
       " 13,\n",
       " 4650,\n",
       " 584,\n",
       " 3006,\n",
       " 13]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_timestamps_text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, _, transcript_end = reader.read()\n",
    "    \n",
    "    print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('00:00:00.000', '00:00:03.090'): 'Men talk more in\\nmale/female interaction', ('00:00:03.090', '00:00:06.110'): \"and they interrupt all the time.\\nIt's partly linked to power.\", ('00:00:06.110', '00:00:09.510'): \"In general people who think they're\\nmore powerful interrupt more.\", ('00:00:10.950', '00:00:14.110'): 'So that would be something that\\na sociolinguist would discover,', ('00:00:14.110', '00:00:16.230'): 'different social groups\\nspeak somewhat differently', ('00:00:16.870', '00:00:19.810'): 'and there are people who look\\nat you know computational linguistics', ('00:00:19.810', '00:00:22.980'): 'and the way you can use language\\nin artificial intelligence,', ('00:00:23.680', '00:00:26.830'): 'teach machines to better\\nbe able to translate.', ('00:00:28.420', '00:00:29.200'): 'Many other areas.'}\n"
     ]
    }
   ],
   "source": [
    "preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, _, transcript_end = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.extend(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.extend(tokenizer.encode(text.strip()))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10418,\n",
       " 1561,\n",
       " 517,\n",
       " 287,\n",
       " 198,\n",
       " 22606,\n",
       " 14,\n",
       " 24724,\n",
       " 10375,\n",
       " 220,\n",
       " 392,\n",
       " 484,\n",
       " 11313,\n",
       " 477,\n",
       " 262,\n",
       " 640,\n",
       " 13,\n",
       " 198,\n",
       " 1026,\n",
       " 338,\n",
       " 11476,\n",
       " 6692,\n",
       " 284,\n",
       " 1176,\n",
       " 13,\n",
       " 220,\n",
       " 818,\n",
       " 2276,\n",
       " 661,\n",
       " 508,\n",
       " 892,\n",
       " 484,\n",
       " 821,\n",
       " 198,\n",
       " 3549,\n",
       " 3665,\n",
       " 11313,\n",
       " 517,\n",
       " 13,\n",
       " 220,\n",
       " 2396,\n",
       " 326,\n",
       " 561,\n",
       " 307,\n",
       " 1223,\n",
       " 326,\n",
       " 198,\n",
       " 64,\n",
       " 1307,\n",
       " 1669,\n",
       " 6680,\n",
       " 396,\n",
       " 561,\n",
       " 7073,\n",
       " 11,\n",
       " 220,\n",
       " 39799,\n",
       " 1919,\n",
       " 2628,\n",
       " 198,\n",
       " 47350,\n",
       " 6454,\n",
       " 10338,\n",
       " 220,\n",
       " 392,\n",
       " 612,\n",
       " 389,\n",
       " 661,\n",
       " 508,\n",
       " 804,\n",
       " 198,\n",
       " 265,\n",
       " 345,\n",
       " 760,\n",
       " 31350,\n",
       " 20280,\n",
       " 3969,\n",
       " 220,\n",
       " 392,\n",
       " 262,\n",
       " 835,\n",
       " 345,\n",
       " 460,\n",
       " 779,\n",
       " 3303,\n",
       " 198,\n",
       " 259,\n",
       " 11666,\n",
       " 4430,\n",
       " 11,\n",
       " 220,\n",
       " 660,\n",
       " 620,\n",
       " 8217,\n",
       " 284,\n",
       " 1365,\n",
       " 198,\n",
       " 1350,\n",
       " 1498,\n",
       " 284,\n",
       " 15772,\n",
       " 13,\n",
       " 220,\n",
       " 7085,\n",
       " 584,\n",
       " 3006,\n",
       " 13]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_timestamps_text_tokens == tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 98)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(no_timestamps_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Men talk more in\\nmale/female interaction and they interrupt all the time.\\nIt's partly linked to power. In general people who think they're\\nmore powerful interrupt more. So that would be something that\\na sociolinguist would discover, different social groups\\nspeak somewhat differently and there are people who look\\nat you know computational linguistics and the way you can use language\\nin artificial intelligence, teach machines to better\\nbe able to translate. Many other areas.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(no_timestamps_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(no_timestamps_text_tokens) == tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Men talk more in\\nmale/female interaction and they interrupt all the time.\\nIt's partly linked to power. In general people who think they're\\nmore powerful interrupt more. So that would be something that\\na sociolinguist would discover, different social groups\\nspeak somewhat differently and there are people who look\\nat you know computational linguistics and the way you can use language\\nin artificial intelligence, teach machines to better\\nbe able to translate. Many other areas.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, _, transcript_end = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_total_tokens=128\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\")\n",
    "    \n",
    "    if np.random.rand() > 0.5:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                elif i < len(transcript) - 1 and i > 0:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                else:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx + [tokenizer.eot]\n",
    "\n",
    "                new_tokens.extend(line_tokens)\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(chain(*tokens))\n",
    "    else:\n",
    "        tokens = list(chain(*tokens))\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_total_tokens=128\n",
      "start='00:00:00.000'\n",
      "end='00:00:03.090'\n",
      "start_ms=0\n",
      "(start_ms // 20)=0\n",
      "3090\n",
      "(end_ms // 20)=154\n",
      "start_token_idx=[50363]\n",
      "end_token_idx=[50517]\n",
      "start='00:00:03.090'\n",
      "end='00:00:06.110'\n",
      "start_ms=3090\n",
      "(start_ms // 20)=154\n",
      "6110\n",
      "(end_ms // 20)=305\n",
      "start_token_idx=[50517]\n",
      "end_token_idx=[50668]\n",
      "start='00:00:06.110'\n",
      "end='00:00:09.510'\n",
      "start_ms=6110\n",
      "(start_ms // 20)=305\n",
      "9510\n",
      "(end_ms // 20)=475\n",
      "start_token_idx=[50668]\n",
      "end_token_idx=[50838]\n",
      "start='00:00:10.950'\n",
      "end='00:00:14.110'\n",
      "start_ms=10950\n",
      "(start_ms // 20)=547\n",
      "14110\n",
      "(end_ms // 20)=705\n",
      "start_token_idx=[50910]\n",
      "end_token_idx=[51068]\n",
      "start='00:00:14.110'\n",
      "end='00:00:16.230'\n",
      "start_ms=14110\n",
      "(start_ms // 20)=705\n",
      "16230\n",
      "(end_ms // 20)=811\n",
      "start_token_idx=[51068]\n",
      "end_token_idx=[51174]\n",
      "start='00:00:16.870'\n",
      "end='00:00:19.810'\n",
      "start_ms=16870\n",
      "(start_ms // 20)=843\n",
      "19810\n",
      "(end_ms // 20)=990\n",
      "start_token_idx=[51206]\n",
      "end_token_idx=[51353]\n",
      "start='00:00:19.810'\n",
      "end='00:00:22.980'\n",
      "start_ms=19810\n",
      "(start_ms // 20)=990\n",
      "22980\n",
      "(end_ms // 20)=1149\n",
      "start_token_idx=[51353]\n",
      "end_token_idx=[51512]\n",
      "start='00:00:23.680'\n",
      "end='00:00:26.830'\n",
      "start_ms=23680\n",
      "(start_ms // 20)=1184\n",
      "26830\n",
      "(end_ms // 20)=1341\n",
      "start_token_idx=[51547]\n",
      "end_token_idx=[51704]\n",
      "start='00:00:28.420'\n",
      "end='00:00:29.200'\n",
      "start_ms=28420\n",
      "(start_ms // 20)=1421\n",
      "29200\n",
      "(end_ms // 20)=1460\n",
      "start_token_idx=[51784]\n",
      "end_token_idx=[51823]\n",
      "tokens=[50257, 50363, 10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 50517, 50517, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 50668, 50668, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 50838, 50910, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 51068, 51068, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 51174, 51206, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 51353, 51353, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 51512, 51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704, 51784, 7085, 584, 3006, 13, 51823, 50256]\n",
      "len(tokens)=127\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\")\n",
    "    \n",
    "    if np.random.rand() > 0:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                elif i < len(transcript) - 1 and i > 0:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "\n",
    "                new_tokens.extend(line_tokens)\n",
    "            unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "            print(f\"{unnorm_start=}\")\n",
    "            norm_next_start = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "            print(f\"{norm_next_start=}\")\n",
    "            next_start_ms = oa.utils.convert_to_milliseconds(norm_next_start)\n",
    "            if next_start_ms > 30000:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "            else:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            new_tokens.append(next_start_token_idx + [tokenizer.eot])\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(chain(*tokens))\n",
    "    else:\n",
    "        tokens = list(chain(*tokens))\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\")\n",
    "    \n",
    "    if np.random.rand() > 0:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                elif i < len(transcript) - 1 and i > 0:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "\n",
    "                new_tokens.extend(line_tokens)\n",
    "            unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "            print(f\"{unnorm_start=}\")\n",
    "            next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "            print(f\"{next_start_ms=}\")\n",
    "            if next_start_ms > 30000:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "            else:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            new_tokens.append(next_start_token_idx + [tokenizer.eot])\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(chain(*tokens))\n",
    "    else:\n",
    "        tokens = list(chain(*tokens))\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\\n\")\n",
    "    \n",
    "    if np.random.rand() > 0:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms=}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                elif i < len(transcript) - 1 and i > 0:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                    \n",
    "                print(f\"{len(line_tokens)=}\")\n",
    "                print(f\"{line_tokens=}\")\n",
    "                new_tokens.extend(line_tokens)\n",
    "                print(f\"{len(new_tokens)=}\\n\")\n",
    "            unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "            print(f\"{unnorm_start=}\")\n",
    "            next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "            print(f\"{next_start_ms=}\")\n",
    "            if next_start_ms > 30000:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "            else:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            new_tokens.extend(next_start_token_idx + [tokenizer.eot])\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(chain(*tokens))\n",
    "    else:\n",
    "        tokens = list(chain(*tokens))\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_total_tokens=128\n",
      "\n",
      "start='00:00:00.000'\n",
      "end='00:00:03.090'\n",
      "start_ms=0\n",
      "(start_ms // 20)=0\n",
      "end_ms=3090\n",
      "(end_ms // 20)=154\n",
      "start_token_idx=[50363]\n",
      "end_token_idx=[50517]\n",
      "len(line_tokens)=13\n",
      "line_tokens=[50257, 50363, 10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 50517]\n",
      "len(new_tokens)=13\n",
      "\n",
      "start='00:00:03.090'\n",
      "end='00:00:06.110'\n",
      "start_ms=3090\n",
      "(start_ms // 20)=154\n",
      "end_ms=6110\n",
      "(end_ms // 20)=305\n",
      "start_token_idx=[50517]\n",
      "end_token_idx=[50668]\n",
      "len(line_tokens)=18\n",
      "line_tokens=[50517, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 50668]\n",
      "len(new_tokens)=31\n",
      "\n",
      "start='00:00:06.110'\n",
      "end='00:00:09.510'\n",
      "start_ms=6110\n",
      "(start_ms // 20)=305\n",
      "end_ms=9510\n",
      "(end_ms // 20)=475\n",
      "start_token_idx=[50668]\n",
      "end_token_idx=[50838]\n",
      "len(line_tokens)=16\n",
      "line_tokens=[50668, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 50838]\n",
      "len(new_tokens)=47\n",
      "\n",
      "start='00:00:10.950'\n",
      "end='00:00:14.110'\n",
      "start_ms=10950\n",
      "(start_ms // 20)=547\n",
      "end_ms=14110\n",
      "(end_ms // 20)=705\n",
      "start_token_idx=[50910]\n",
      "end_token_idx=[51068]\n",
      "len(line_tokens)=18\n",
      "line_tokens=[50910, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 51068]\n",
      "len(new_tokens)=65\n",
      "\n",
      "start='00:00:14.110'\n",
      "end='00:00:16.230'\n",
      "start_ms=14110\n",
      "(start_ms // 20)=705\n",
      "end_ms=16230\n",
      "(end_ms // 20)=811\n",
      "start_token_idx=[51068]\n",
      "end_token_idx=[51174]\n",
      "len(line_tokens)=10\n",
      "line_tokens=[51068, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 51174]\n",
      "len(new_tokens)=75\n",
      "\n",
      "start='00:00:16.870'\n",
      "end='00:00:19.810'\n",
      "start_ms=16870\n",
      "(start_ms // 20)=843\n",
      "end_ms=19810\n",
      "(end_ms // 20)=990\n",
      "start_token_idx=[51206]\n",
      "end_token_idx=[51353]\n",
      "len(line_tokens)=16\n",
      "line_tokens=[51206, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 51353]\n",
      "len(new_tokens)=91\n",
      "\n",
      "start='00:00:19.810'\n",
      "end='00:00:22.980'\n",
      "start_ms=19810\n",
      "(start_ms // 20)=990\n",
      "end_ms=22980\n",
      "(end_ms // 20)=1149\n",
      "start_token_idx=[51353]\n",
      "end_token_idx=[51512]\n",
      "len(line_tokens)=15\n",
      "line_tokens=[51353, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 51512]\n",
      "len(new_tokens)=106\n",
      "\n",
      "start='00:00:23.680'\n",
      "end='00:00:26.830'\n",
      "start_ms=23680\n",
      "(start_ms // 20)=1184\n",
      "end_ms=26830\n",
      "(end_ms // 20)=1341\n",
      "start_token_idx=[51547]\n",
      "end_token_idx=[51704]\n",
      "len(line_tokens)=14\n",
      "line_tokens=[51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704]\n",
      "len(new_tokens)=120\n",
      "\n",
      "start='00:00:28.420'\n",
      "end='00:00:29.200'\n",
      "start_ms=28420\n",
      "(start_ms // 20)=1421\n",
      "end_ms=29200\n",
      "(end_ms // 20)=1460\n",
      "start_token_idx=[51784]\n",
      "end_token_idx=[51823]\n",
      "len(line_tokens)=14\n",
      "line_tokens=[51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704]\n",
      "len(new_tokens)=134\n",
      "\n",
      "unnorm_start='00:00:29.100'\n",
      "next_start_ms=29200\n",
      "tokens=[50257, 50363, 10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 50517, 50517, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 50668, 50668, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 50838, 50910, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 51068, 51068, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 51174, 51206, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 51353, 51353, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 51512, 51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704, 51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704, 51823, 50256]\n",
      "len(tokens)=136\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer, text_timestamp=\"00:00:29,100_00:00:58,300\", next_start=\"00:00:58.300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\\n\")\n",
    "    print(f\"{([len(token_group) for token_group in tokens])=}\")\n",
    "    \n",
    "    if np.random.rand() > 0:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms=}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                else:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                    \n",
    "                print(f\"{len(line_tokens)=}\")\n",
    "                print(f\"{line_tokens=}\")\n",
    "                new_tokens.extend(line_tokens)\n",
    "                print(f\"{len(new_tokens)=}\\n\")\n",
    "            unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "            print(f\"{unnorm_start=}\")\n",
    "            next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "            print(f\"{next_start_ms=}\")\n",
    "            if next_start_ms > 30000:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "            else:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            new_tokens.extend(next_start_token_idx + [tokenizer.eot])\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(chain(*tokens))\n",
    "    else:\n",
    "        tokens = list(chain(*tokens))\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_total_tokens=128\n",
      "\n",
      "([len(token_group) for token_group in tokens])=[10, 16, 14, 16, 8, 14, 13, 12, 4]\n",
      "start='00:00:00.000'\n",
      "end='00:00:03.090'\n",
      "start_ms=0\n",
      "(start_ms // 20)=0\n",
      "end_ms=3090\n",
      "(end_ms // 20)=154\n",
      "start_token_idx=[50363]\n",
      "end_token_idx=[50517]\n",
      "len(line_tokens)=13\n",
      "line_tokens=[50257, 50363, 10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 50517]\n",
      "len(new_tokens)=13\n",
      "\n",
      "start='00:00:03.090'\n",
      "end='00:00:06.110'\n",
      "start_ms=3090\n",
      "(start_ms // 20)=154\n",
      "end_ms=6110\n",
      "(end_ms // 20)=305\n",
      "start_token_idx=[50517]\n",
      "end_token_idx=[50668]\n",
      "len(line_tokens)=18\n",
      "line_tokens=[50517, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 50668]\n",
      "len(new_tokens)=31\n",
      "\n",
      "start='00:00:06.110'\n",
      "end='00:00:09.510'\n",
      "start_ms=6110\n",
      "(start_ms // 20)=305\n",
      "end_ms=9510\n",
      "(end_ms // 20)=475\n",
      "start_token_idx=[50668]\n",
      "end_token_idx=[50838]\n",
      "len(line_tokens)=16\n",
      "line_tokens=[50668, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 50838]\n",
      "len(new_tokens)=47\n",
      "\n",
      "start='00:00:10.950'\n",
      "end='00:00:14.110'\n",
      "start_ms=10950\n",
      "(start_ms // 20)=547\n",
      "end_ms=14110\n",
      "(end_ms // 20)=705\n",
      "start_token_idx=[50910]\n",
      "end_token_idx=[51068]\n",
      "len(line_tokens)=18\n",
      "line_tokens=[50910, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 51068]\n",
      "len(new_tokens)=65\n",
      "\n",
      "start='00:00:14.110'\n",
      "end='00:00:16.230'\n",
      "start_ms=14110\n",
      "(start_ms // 20)=705\n",
      "end_ms=16230\n",
      "(end_ms // 20)=811\n",
      "start_token_idx=[51068]\n",
      "end_token_idx=[51174]\n",
      "len(line_tokens)=10\n",
      "line_tokens=[51068, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 51174]\n",
      "len(new_tokens)=75\n",
      "\n",
      "start='00:00:16.870'\n",
      "end='00:00:19.810'\n",
      "start_ms=16870\n",
      "(start_ms // 20)=843\n",
      "end_ms=19810\n",
      "(end_ms // 20)=990\n",
      "start_token_idx=[51206]\n",
      "end_token_idx=[51353]\n",
      "len(line_tokens)=16\n",
      "line_tokens=[51206, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 51353]\n",
      "len(new_tokens)=91\n",
      "\n",
      "start='00:00:19.810'\n",
      "end='00:00:22.980'\n",
      "start_ms=19810\n",
      "(start_ms // 20)=990\n",
      "end_ms=22980\n",
      "(end_ms // 20)=1149\n",
      "start_token_idx=[51353]\n",
      "end_token_idx=[51512]\n",
      "len(line_tokens)=15\n",
      "line_tokens=[51353, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 51512]\n",
      "len(new_tokens)=106\n",
      "\n",
      "start='00:00:23.680'\n",
      "end='00:00:26.830'\n",
      "start_ms=23680\n",
      "(start_ms // 20)=1184\n",
      "end_ms=26830\n",
      "(end_ms // 20)=1341\n",
      "start_token_idx=[51547]\n",
      "end_token_idx=[51704]\n",
      "len(line_tokens)=14\n",
      "line_tokens=[51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704]\n",
      "len(new_tokens)=120\n",
      "\n",
      "start='00:00:28.420'\n",
      "end='00:00:29.200'\n",
      "start_ms=28420\n",
      "(start_ms // 20)=1421\n",
      "end_ms=29200\n",
      "(end_ms // 20)=1460\n",
      "start_token_idx=[51784]\n",
      "end_token_idx=[51823]\n",
      "len(line_tokens)=6\n",
      "line_tokens=[51784, 7085, 584, 3006, 13, 51823]\n",
      "len(new_tokens)=126\n",
      "\n",
      "unnorm_start='00:00:29.100'\n",
      "next_start_ms=29200\n",
      "tokens=[50257, 50363, 10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 50517, 50517, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 50668, 50668, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 50838, 50910, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 51068, 51068, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 51174, 51206, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 51353, 51353, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 51512, 51547, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 51704, 51784, 7085, 584, 3006, 13, 51823, 51823, 50256]\n",
      "len(tokens)=128\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer, text_timestamp=\"00:00:29,100_00:00:58,300\", next_start=\"00:00:58.300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftranscript|><|0.00|>Men talk more in\n",
      "male/female interaction <|3.08|><|3.08|>and they interrupt all the time.\n",
      "It's partly linked to power. <|6.10|><|6.10|>In general people who think they're\n",
      "more powerful interrupt more. <|9.50|><|10.94|>So that would be something that\n",
      "a sociolinguist would discover, <|14.10|><|14.10|>different social groups\n",
      "speak somewhat differently <|16.22|><|16.86|>and there are people who look\n",
      "at you know computational linguistics <|19.80|><|19.80|>and the way you can use language\n",
      "in artificial intelligence, <|22.98|><|23.68|>teach machines to better\n",
      "be able to translate. <|26.82|><|28.42|>Many other areas.<|29.20|><|29.20|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode_with_timestamps(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftranscript|>Men talk more in\n",
      "male/female interaction and they interrupt all the time.\n",
      "It's partly linked to power. In general people who think they're\n",
      "more powerful interrupt more. So that would be something that\n",
      "a sociolinguist would discover, different social groups\n",
      "speak somewhat differently and there are people who look\n",
      "at you know computational linguistics and the way you can use language\n",
      "in artificial intelligence, teach machines to better\n",
      "be able to translate. Many other areas.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\\n\")\n",
    "    print(f\"{([len(token_group) for token_group in tokens])=}\")\n",
    "    \n",
    "    if np.random.rand() > 2:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms=}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                else:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                    \n",
    "                print(f\"{len(line_tokens)=}\")\n",
    "                print(f\"{line_tokens=}\")\n",
    "                new_tokens.extend(line_tokens)\n",
    "                print(f\"{len(new_tokens)=}\\n\")\n",
    "            unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "            print(f\"{unnorm_start=}\")\n",
    "            next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "            print(f\"{next_start_ms=}\")\n",
    "            if next_start_ms > 30000:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "            else:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            new_tokens.extend(next_start_token_idx + [tokenizer.eot])\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(chain(*tokens))\n",
    "    else:\n",
    "        tokens = list(chain(*tokens))\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_total_tokens=128\n",
      "\n",
      "([len(token_group) for token_group in tokens])=[10, 16, 14, 16, 8, 14, 13, 12, 4]\n",
      "tokens=[10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 7085, 584, 3006, 13]\n",
      "len(tokens)=107\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer, text_timestamp=\"00:00:29,100_00:00:58,300\", next_start=\"00:00:58.300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Men talk more in\n",
      "male/female interaction and they interrupt all the time.\n",
      "It's partly linked to power. In general people who think they're\n",
      "more powerful interrupt more. So that would be something that\n",
      "a sociolinguist would discover, different social groups\n",
      "speak somewhat differently and there are people who look\n",
      "at you know computational linguistics and the way you can use language\n",
      "in artificial intelligence, teach machines to better\n",
      "be able to translate. Many other areas.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = []\n",
    "    for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "        if i < len(transcript) - 1:\n",
    "            tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "        else:\n",
    "            tokens.append(tokenizer.encode(text.strip()))\n",
    "            \n",
    "    num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "    print(f\"{num_timestamp_tokens=}\")\n",
    "    num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "    print(f\"{num_text_tokens=}\")\n",
    "    num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "    print(f\"{num_total_tokens=}\\n\")\n",
    "    print(f\"{([len(token_group) for token_group in tokens])=}\")\n",
    "    \n",
    "    if np.random.rand() > 2:\n",
    "        if num_total_tokens <= 448:\n",
    "            new_tokens = []\n",
    "            for i, timestamps in enumerate(transcript.keys()):\n",
    "                start, end = timestamps\n",
    "                print(f\"{start=}\")\n",
    "                print(f\"{end=}\")\n",
    "                start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                print(f\"{start_ms=}\")\n",
    "                print(f\"{(start_ms // 20)=}\")\n",
    "                end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                print(f\"{end_ms=}\")\n",
    "                print(f\"{(end_ms // 20)=}\")\n",
    "                start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                print(f\"{start_token_idx=}\")\n",
    "                end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                print(f\"{end_token_idx=}\")\n",
    "\n",
    "                if i == 0:\n",
    "                    line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                else:\n",
    "                    line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                    \n",
    "                print(f\"{len(line_tokens)=}\")\n",
    "                print(f\"{line_tokens=}\")\n",
    "                new_tokens.extend(line_tokens)\n",
    "                print(f\"{len(new_tokens)=}\\n\")\n",
    "            unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "            print(f\"{unnorm_start=}\")\n",
    "            next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "            print(f\"{next_start_ms=}\")\n",
    "            if next_start_ms > 30000:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "            else:\n",
    "                next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            new_tokens.extend(next_start_token_idx + [tokenizer.eot])\n",
    "            tokens = new_tokens\n",
    "        else:\n",
    "            tokens = list(tokenizer.sot_sequence_including_notimestamps) + list(chain(*tokens)) + [tokenizer.eot]\n",
    "    else:\n",
    "        tokens = list(tokenizer.sot_sequence_including_notimestamps) + list(chain(*tokens)) + [tokenizer.eot]\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_total_tokens=128\n",
      "\n",
      "([len(token_group) for token_group in tokens])=[10, 16, 14, 16, 8, 14, 13, 12, 4]\n",
      "tokens=[50257, 50362, 10418, 1561, 517, 287, 198, 22606, 14, 24724, 10375, 220, 392, 484, 11313, 477, 262, 640, 13, 198, 1026, 338, 11476, 6692, 284, 1176, 13, 220, 818, 2276, 661, 508, 892, 484, 821, 198, 3549, 3665, 11313, 517, 13, 220, 2396, 326, 561, 307, 1223, 326, 198, 64, 1307, 1669, 6680, 396, 561, 7073, 11, 220, 39799, 1919, 2628, 198, 47350, 6454, 10338, 220, 392, 612, 389, 661, 508, 804, 198, 265, 345, 760, 31350, 20280, 3969, 220, 392, 262, 835, 345, 460, 779, 3303, 198, 259, 11666, 4430, 11, 220, 660, 620, 8217, 284, 1365, 198, 1350, 1498, 284, 15772, 13, 220, 7085, 584, 3006, 13, 50256]\n",
      "len(tokens)=110\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=sample, transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\", tokenizer=tokenizer, text_timestamp=\"00:00:29,100_00:00:58,300\", next_start=\"00:00:58.300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftranscript|><|notimestamps|>Men talk more in\n",
      "male/female interaction and they interrupt all the time.\n",
      "It's partly linked to power. In general people who think they're\n",
      "more powerful interrupt more. So that would be something that\n",
      "a sociolinguist would discover, different social groups\n",
      "speak somewhat differently and there are people who look\n",
      "at you know computational linguistics and the way you can use language\n",
      "in artificial intelligence, teach machines to better\n",
      "be able to translate. Many other areas.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = [] \n",
    "    if not transcript:\n",
    "        unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "        print(f\"{unnorm_start=}\")\n",
    "        next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "        print(f\"{next_start_ms=}\")\n",
    "        if next_start_ms > 30000:\n",
    "            next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "        else:\n",
    "            next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            \n",
    "        if np.random.rand() > 0:\n",
    "            tokens = [tokenizer.sot_sequence[0]] + [tokenizer.timestamp_begin] + [tokenizer.no_speech] + next_start_token_idx + [tokenizer.eot]\n",
    "        else:\n",
    "            tokens = list(tokenizer.sot_sequence_including_notimestamps) + [tokenizer.no_speech] + [tokenizer.eot]\n",
    "    else:        \n",
    "        for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "            if i < len(transcript) - 1:\n",
    "                tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "            else:\n",
    "                tokens.append(tokenizer.encode(text.strip()))\n",
    "                \n",
    "        num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "        print(f\"{num_timestamp_tokens=}\")\n",
    "        num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "        print(f\"{num_text_tokens=}\")\n",
    "        num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "        print(f\"{num_total_tokens=}\\n\")\n",
    "        print(f\"{([len(token_group) for token_group in tokens])=}\")\n",
    "        \n",
    "        if np.random.rand() > 0:\n",
    "            if num_total_tokens <= 448:\n",
    "                new_tokens = []\n",
    "                for i, timestamps in enumerate(transcript.keys()):\n",
    "                    start, end = timestamps\n",
    "                    print(f\"{start=}\")\n",
    "                    print(f\"{end=}\")\n",
    "                    start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                    print(f\"{start_ms=}\")\n",
    "                    print(f\"{(start_ms // 20)=}\")\n",
    "                    end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                    print(f\"{end_ms=}\")\n",
    "                    print(f\"{(end_ms // 20)=}\")\n",
    "                    start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                    print(f\"{start_token_idx=}\")\n",
    "                    end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                    print(f\"{end_token_idx=}\")\n",
    "\n",
    "                    if i == 0:\n",
    "                        line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                    else:\n",
    "                        line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                        \n",
    "                    print(f\"{len(line_tokens)=}\")\n",
    "                    print(f\"{line_tokens=}\")\n",
    "                    new_tokens.extend(line_tokens)\n",
    "                    print(f\"{len(new_tokens)=}\\n\")\n",
    "                unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "                print(f\"{unnorm_start=}\")\n",
    "                next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "                print(f\"{next_start_ms=}\")\n",
    "                if next_start_ms > 30000:\n",
    "                    next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "                else:\n",
    "                    next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "                new_tokens.extend(next_start_token_idx + [tokenizer.eot])\n",
    "                tokens = new_tokens\n",
    "            else:\n",
    "                tokens = list(tokenizer.sot_sequence_including_notimestamps) + list(chain(*tokens)) + [tokenizer.eot]\n",
    "        else:\n",
    "            tokens = list(tokenizer.sot_sequence_including_notimestamps) + list(chain(*tokens)) + [tokenizer.eot]\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unnorm_start='00:00:28.050'\n",
      "next_start_ms=1050\n",
      "tokens=[50257, 50363, 50361, 50415, 50256]\n",
      "len(tokens)=5\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=\"\", transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:28,050_00:00:29,100.srt\", tokenizer=tokenizer, text_timestamp=\"00:00:28,050_00:00:29,100\", next_start=\"00:00:29.100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftranscript|><|0.00|><|nospeech|><|1.04|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode_with_timestamps(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from itertools import chain\n",
    "n_text_ctx = 448\n",
    "def preprocess_text(\n",
    "    transcript_string: str,\n",
    "    transcript_file: str,\n",
    "    tokenizer: whisper.tokenizer.Tokenizer,\n",
    "    text_timestamp: str,\n",
    "    next_start: str,\n",
    ") -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Preprocesses the text data for the model.\n",
    "\n",
    "    Reads in the transcript file and extracts the text data. Tokenizes the text data and pads it to the context length.\n",
    "\n",
    "    Args:\n",
    "        transcript_file: The path to the transcript file\n",
    "        tokenizer: The tokenizer to use for encoding the text data\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the transcript file, the input text tensor, the target text tensor, and the padding mask\n",
    "    \"\"\"\n",
    "    # transcript -> text\n",
    "    reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "    transcript, *_ = reader.read()\n",
    "    tokens = [] \n",
    "    if not transcript:\n",
    "        unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "        print(f\"{unnorm_start=}\")\n",
    "        next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "        print(f\"{next_start_ms=}\")\n",
    "        if next_start_ms > 30000:\n",
    "            next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "        else:\n",
    "            next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "            \n",
    "        if np.random.rand() > 1:\n",
    "            tokens = [tokenizer.sot_sequence[0]] + [tokenizer.timestamp_begin] + [tokenizer.no_speech] + next_start_token_idx + [tokenizer.eot]\n",
    "        else:\n",
    "            tokens = list(tokenizer.sot_sequence_including_notimestamps) + [tokenizer.no_speech] + [tokenizer.eot]\n",
    "    else:        \n",
    "        for i, (timestamps, text) in enumerate(transcript.items()):\n",
    "            if i < len(transcript) - 1:\n",
    "                tokens.append(tokenizer.encode(text.strip() + \" \"))\n",
    "            else:\n",
    "                tokens.append(tokenizer.encode(text.strip()))\n",
    "                \n",
    "        num_timestamp_tokens = (len(transcript) * 2) + 1 # next_start timestamp\n",
    "        print(f\"{num_timestamp_tokens=}\")\n",
    "        num_text_tokens = sum([len(token_group) for token_group in tokens])\n",
    "        print(f\"{num_text_tokens=}\")\n",
    "        num_total_tokens = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "        print(f\"{num_total_tokens=}\\n\")\n",
    "        print(f\"{([len(token_group) for token_group in tokens])=}\")\n",
    "        \n",
    "        if np.random.rand() > 0:\n",
    "            if num_total_tokens <= 448:\n",
    "                new_tokens = []\n",
    "                for i, timestamps in enumerate(transcript.keys()):\n",
    "                    start, end = timestamps\n",
    "                    print(f\"{start=}\")\n",
    "                    print(f\"{end=}\")\n",
    "                    start_ms = oa.utils.convert_to_milliseconds(start)\n",
    "                    print(f\"{start_ms=}\")\n",
    "                    print(f\"{(start_ms // 20)=}\")\n",
    "                    end_ms = oa.utils.convert_to_milliseconds(end)\n",
    "                    print(f\"{end_ms=}\")\n",
    "                    print(f\"{(end_ms // 20)=}\")\n",
    "                    start_token_idx = [tokenizer.timestamp_begin + (start_ms // 20)]\n",
    "                    print(f\"{start_token_idx=}\")\n",
    "                    end_token_idx = [tokenizer.timestamp_begin + (end_ms // 20)]\n",
    "                    print(f\"{end_token_idx=}\")\n",
    "\n",
    "                    if i == 0:\n",
    "                        line_tokens = [tokenizer.sot_sequence[0]] + start_token_idx + tokens[i] + end_token_idx\n",
    "                    else:\n",
    "                        line_tokens = start_token_idx + tokens[i] + end_token_idx\n",
    "                        \n",
    "                    print(f\"{len(line_tokens)=}\")\n",
    "                    print(f\"{line_tokens=}\")\n",
    "                    new_tokens.extend(line_tokens)\n",
    "                    print(f\"{len(new_tokens)=}\\n\")\n",
    "                unnorm_start = text_timestamp.split(\"_\")[0].replace(\",\", \".\")\n",
    "                print(f\"{unnorm_start=}\")\n",
    "                next_start_ms = oa.utils.calculate_difference(unnorm_start, next_start)\n",
    "                print(f\"{next_start_ms=}\")\n",
    "                if next_start_ms > 30000:\n",
    "                    next_start_token_idx = [tokenizer.timestamp_begin + (30000 // 20)]\n",
    "                else:\n",
    "                    next_start_token_idx = [tokenizer.timestamp_begin + (next_start_ms // 20)]\n",
    "                new_tokens.extend(next_start_token_idx + [tokenizer.eot])\n",
    "                tokens = new_tokens\n",
    "            else:\n",
    "                tokens = list(tokenizer.sot_sequence_including_notimestamps) + list(chain(*tokens)) + [tokenizer.eot]\n",
    "        else:\n",
    "            tokens = list(tokenizer.sot_sequence_including_notimestamps) + list(chain(*tokens)) + [tokenizer.eot]\n",
    "    print(f\"{tokens=}\")\n",
    "    print(f\"{len(tokens)=}\")\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unnorm_start='00:00:28.050'\n",
      "next_start_ms=1050\n",
      "tokens=[50257, 50362, 50361, 50256]\n",
      "len(tokens)=4\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess_text(transcript_string=\"\", transcript_file=\"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:28,050_00:00:29,100.srt\", tokenizer=tokenizer, text_timestamp=\"00:00:28,050_00:00:29,100\", next_start=\"00:00:29.100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftranscript|><|notimestamps|><|nospeech|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "def over_ctx_len(\n",
    "    timestamps: List, transcript: Optional[Dict], language: Optional[str]\n",
    ") -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Check if transcript text exceeds model context length\n",
    "\n",
    "    Check if the total number of tokens in the transcript text exceeds the model context length\n",
    "\n",
    "    Args:\n",
    "        timestamps: List of timestamps\n",
    "        transcript: Transcript as a dictionary\n",
    "\n",
    "    Returns:\n",
    "        True if the transcript text exceeds the model context length, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if language is None:\n",
    "            tokenizer = get_tokenizer(multilingual=False)\n",
    "        else:\n",
    "            tokenizer = get_tokenizer(language=language, multilingual=True)\n",
    "\n",
    "        text_tokens = [\n",
    "            (\n",
    "                tokenizer.encode(transcript[timestamp].strip() + \" \")\n",
    "                if i < len(timestamps) - 1\n",
    "                else tokenizer.encode(transcript[timestamp].strip())\n",
    "            )\n",
    "            for i, timestamp in enumerate(timestamps)\n",
    "        ]\n",
    "        \n",
    "        # text_tokens = list(chain(*text_tokens))\n",
    "        \n",
    "        num_timestamp_tokens = (len(timestamps) * 2) + 1 # next_start timestamp\n",
    "        print(f\"{num_timestamp_tokens=}\")\n",
    "        num_text_tokens = sum([len(token_group) for token_group in text_tokens])\n",
    "        print(f\"{num_text_tokens=}\")\n",
    "        num_tokens_ts_mode = num_timestamp_tokens + num_text_tokens + 2 # sot + eot\n",
    "        print(f\"{num_tokens_ts_mode=}\")\n",
    "        num_tokens_no_ts_mode = num_text_tokens + 3 # sot + notimestamps + eot\n",
    "        print(f\"{num_tokens_no_ts_mode=}\")\n",
    "        \n",
    "        if num_tokens_ts_mode > 448 and num_tokens_no_ts_mode > 448:\n",
    "            return True, None\n",
    "        elif num_tokens_ts_mode > 448 and num_tokens_no_ts_mode <= 448:\n",
    "            return False, {\"ts_mode\": False, \"no_ts_mode\": True}\n",
    "        elif num_tokens_ts_mode <= 448 and num_tokens_no_ts_mode > 448:\n",
    "            return False, {\"ts_mode\": True, \"no_ts_mode\": False}\n",
    "        else:\n",
    "            return False, {\"ts_mode\": True, \"no_ts_mode\": True}\n",
    "    except RuntimeError:\n",
    "        return True, \"error\"\n",
    "    except Exception as e:\n",
    "        return True, \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timestamp_tokens=19\n",
      "num_text_tokens=107\n",
      "num_tokens_ts_mode=128\n",
      "num_tokens_no_ts_mode=110\n"
     ]
    }
   ],
   "source": [
    "transcript_string = sample\n",
    "transcript_file = \"/weka/huongn/oa_seg/00000000/7HOsQDD1Res/00:00:29,100_00:00:58,300.srt\"\n",
    "reader = oa.utils.TranscriptReader(\n",
    "        transcript_string=transcript_string,\n",
    "        file_path=None,\n",
    "        ext=transcript_file.split(\".\")[-1],\n",
    "    )\n",
    "transcript, *_ = reader.read()\n",
    "temp, res = over_ctx_len(timestamps=list(transcript.keys()), transcript=transcript, language=None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
