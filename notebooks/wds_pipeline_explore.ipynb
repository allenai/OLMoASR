{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e10f9-478c-49d1-8739-b780d04b2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import wandb\n",
    "from typing import List, Tuple, Union, Optional, Literal, Dict\n",
    "import time\n",
    "import jiwer\n",
    "from fire import Fire\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import whisper\n",
    "from whisper import audio, DecodingOptions\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "from whisper.tokenizer import get_tokenizer\n",
    "import whisper.tokenizer\n",
    "from olmoasr.config.model_dims import VARIANT_TO_DIMS, ModelDimensions\n",
    "import olmoasr as oa\n",
    "\n",
    "import webdataset as wds\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d885a1-a45f-465c-95ef-ff2380254f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio_bytes(audio_bytes: bytes) -> np.ndarray:\n",
    "    bytes_io = BytesIO(audio_bytes)\n",
    "    audio_arr = np.load(bytes_io)\n",
    "\n",
    "    return audio_arr\n",
    "\n",
    "def decode_text_bytes(text_bytes: bytes) -> str:\n",
    "    transcript_str = text_bytes.decode(\"utf-8\")\n",
    "\n",
    "    return transcript_str\n",
    "\n",
    "def decode_sample(sample: Dict[str, bytes]) -> Tuple[np.ndarray, str]:\n",
    "    file_path = os.path.join(sample[\"__url__\"], sample[\"__key__\"])\n",
    "    audio_path = file_path + \".m4a\"\n",
    "    text_path = file_path + \".srt\"\n",
    "    audio_bytes = sample[\"npy\"]\n",
    "    text_bytes = sample[\"srt\"]\n",
    "    audio_arr = decode_audio_bytes(audio_bytes)\n",
    "    transcript_str = decode_text_bytes(text_bytes)\n",
    "\n",
    "    return audio_path, audio_arr, text_path, transcript_str\n",
    "\n",
    "def preprocess_audio(audio_arr: np.ndarray) -> torch.Tensor:\n",
    "    audio_arr = audio_arr.astype(np.float32) / 32768.0\n",
    "    audio_arr = audio.pad_or_trim(audio_arr)\n",
    "    mel_spec = audio.log_mel_spectrogram(audio_arr)\n",
    "\n",
    "    return mel_spec, audio_arr\n",
    "\n",
    "def preprocess_text(transcript_string: str, tokenizer: whisper.tokenizer.Tokenizer, n_text_ctx: int) -> Tuple[str, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    reader = oa.utils.TranscriptReader(transcript_string=transcript_string, ext=\"srt\")\n",
    "    transcript, *_ = reader.read()\n",
    "    \n",
    "    if not transcript:\n",
    "        text_tokens = [tokenizer.no_speech]\n",
    "    else:\n",
    "        transcript_text = reader.extract_text(transcript=transcript)\n",
    "\n",
    "        text_tokens = tokenizer.encode(transcript_text)\n",
    "\n",
    "    text_tokens = list(tokenizer.sot_sequence_including_notimestamps) + text_tokens\n",
    "\n",
    "    text_tokens.append(tokenizer.eot)\n",
    "\n",
    "    # offset\n",
    "    text_input = text_tokens[:-1]\n",
    "    text_y = text_tokens[1:]\n",
    "\n",
    "    padding_mask = torch.zeros((n_text_ctx, n_text_ctx))\n",
    "    padding_mask[:, len(text_input) :] = -float(\"inf\")\n",
    "\n",
    "    text_input = np.pad(\n",
    "        text_input,\n",
    "        pad_width=(0, n_text_ctx - len(text_input)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=51864,\n",
    "    )\n",
    "    text_y = np.pad(\n",
    "        text_y,\n",
    "        pad_width=(0, n_text_ctx - len(text_y)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=51864,\n",
    "    )\n",
    "\n",
    "    text_input = torch.tensor(text_input, dtype=torch.long)\n",
    "    text_y = torch.tensor(text_y, dtype=torch.long)\n",
    "\n",
    "    return text_input, text_y, padding_mask\n",
    "    \n",
    "def preprocess(sample, n_text_ctx: int):\n",
    "    tokenizer = get_tokenizer(multilingual=False)\n",
    "    audio_path, audio_arr, text_path, transcript_str = decode_sample(sample)\n",
    "    audio_input, padded_audio_arr = preprocess_audio(audio_arr)\n",
    "    text_input, text_y, padding_mask = preprocess_text(transcript_str, tokenizer, n_text_ctx)\n",
    "\n",
    "    return audio_path, text_path, padded_audio_arr, audio_input, text_input, text_y, padding_mask\n",
    "\n",
    "def shuffle_shards(shards: str) -> List[str]:\n",
    "    start_train_shard, end_train_shard = [int(shard_idx) for shard_idx in shards.split(\"{\")[-1].split(\"}\")[0].split(\"..\")]\n",
    "    rng = np.random.default_rng(42)\n",
    "    shards_list = np.array(range(start_train_shard, end_train_shard + 1))\n",
    "    rng.shuffle(shards_list)\n",
    "    shuffled_shards_list = [f\"data/tars/{shard_idx:08d}.tar\" for shard_idx in shards_list]\n",
    "    \n",
    "    return shuffled_shards_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4c0a9-e541-4f42-a8b0-3d1bc75b5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset(\"data/tars/{000000..000019}.tar\").map(lambda sample: preprocess(sample, 448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372e160c-ada9-404a-ac1b-23677599fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, drop_last=False)\n",
    "for batch in dataloader:\n",
    "    audio_path, text_path, padded_audio_arr, audio_input, text_input, text_y, padding_mask = batch\n",
    "    print(audio_input.shape, text_input.shape, text_y.shape, padding_mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.WebDataset(\"data/tars/{000000..000019}.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample[\"__key__\"])\n",
    "    print(sample[\"__url__\"])\n",
    "    if count == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    # at this point we have an iterator over all the shards\n",
    "    wds.detshuffle(bufsize=1000, initial=100, seed=42),\n",
    "\n",
    "    # add wds.split_by_node here if you are using multiple nodes\n",
    "    wds.split_by_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "    # add wds.split_by_node here if you are using multiple nodes\n",
    "    wds.split_by_worker,\n",
    "    wds.detshuffle(bufsize=1000, initial=100, seed=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "    # add wds.split_by_node here if you are using multiple nodes\n",
    "    wds.split_by_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.decode(decode_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.decode(wds.handle_extension(\".npy\", decode_audio_bytes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.decode(wds.handle_extension(\".npy\", decode_audio_bytes)),\n",
    "    wds.decode(wds.handle_extension(\".srt\", decode_text_bytes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.map(decode_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    temp_sample = sample\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in temp_sample:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample, n_text_ctx):\n",
    "    tokenizer = get_tokenizer(multilingual=False)\n",
    "    audio_path = sample[0]\n",
    "    audio_arr = sample[1]\n",
    "    text_path = sample[2]\n",
    "    transcript_str = sample[3]\n",
    "    audio_input, padded_audio_arr = preprocess_audio(audio_arr)\n",
    "    text_input, text_y, padding_mask = preprocess_text(transcript_str, tokenizer, n_text_ctx)\n",
    "\n",
    "    return audio_path, text_path, padded_audio_arr, audio_input, text_input, text_y, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.map(decode_sample),\n",
    "    wds.map(lambda sample: preprocess(sample, 448)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(multilingual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sample, tokenizer, n_text_ctx):\n",
    "    audio_path = sample[0]\n",
    "    audio_arr = sample[1]\n",
    "    text_path = sample[2]\n",
    "    transcript_str = sample[3]\n",
    "    audio_input, padded_audio_arr = preprocess_audio(audio_arr)\n",
    "    text_input, text_y, padding_mask = preprocess_text(transcript_str, tokenizer, n_text_ctx)\n",
    "\n",
    "    return audio_path, text_path, padded_audio_arr, audio_input, text_input, text_y, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_text_ctx = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.map(decode_sample),\n",
    "    wds.map(lambda sample: preprocess(sample, tokenizer, n_text_ctx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.map(decode_sample),\n",
    "    wds.map(lambda sample: preprocess(sample, tokenizer, n_text_ctx)),\n",
    "    wds.batched(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sample in dataset:\n",
    "    count += 1\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wds.WebLoader(dataset, batch_size=None, shuffle=False, pin_memory=True, num_workers=4, drop_last=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = wds.WebLoader(dataset, batch_size=None, shuffle=False, pin_memory=True, num_workers=4, drop_last=False, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(batch[\"audio_files\"])\n",
    "    print(batch[\"transcript_files\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(batch[0])\n",
    "    print(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "\n",
    "    # at this point, we have an iterator over the shards assigned to each worker\n",
    "    wds.tarfile_to_samples(),\n",
    "\n",
    "    # this shuffles the samples in memory\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.map(decode_sample),\n",
    "    wds.map(lambda sample: preprocess(sample, tokenizer, n_text_ctx)),\n",
    "    wds.shuffle(bufsize=1000, initial=100),\n",
    "    wds.batched(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import webdataset as wds\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    dataset = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"data/tars/{000000..000019}.tar\"),\n",
    "\n",
    "    wds.split_by_worker,\n",
    "\n",
    "    wds.shuffle(bufsize=1000, initial=100))\n",
    "\n",
    "    for sample in dataset:\n",
    "        print(rank, world_size)\n",
    "        print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = torch.cuda.device_count()\n",
    "mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(batch[0])\n",
    "    print(batch[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.with_epoch(30764 // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 3):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if batch_idx == 0:\n",
    "            print(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 3):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if batch_idx == 0:\n",
    "            print(batch[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open_whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
